{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-welcome",
   "metadata": {},
   "source": [
    "Hello **Everyone** !  \n",
    "Welcome to the **second part of Day 2** of our AI pool !  \n",
    "This afternoon, we will explore how to make an AI model smarter by giving it access to external knowledge.  \n",
    "\n",
    "Our goal : build a system that can answer questions about **your own documents** (PDFs, texts, etc.) with accurate information.  \n",
    "Does that sound useful ? Let's dive in !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "context-problem",
   "metadata": {},
   "source": [
    "**But wait... why do we need this ?**\n",
    "\n",
    "Remember this morning ? We fine-tuned a model to give us false capitals. The model \"learned\" these false facts.  \n",
    "But there is a problem : **what if the information changes ?** What if we have thousands of documents that update every day ?\n",
    "\n",
    "Fine-tuning every time is expensive and slow. We need a smarter approach : **RAG** (Retrieval Augmented Generation).  \n",
    "\n",
    "Think of it like this :\n",
    "- **Fine-tuning** = Teaching a student new facts by heart (slow, expensive)\n",
    "- **RAG** = Giving the student access to a library and teaching them how to search (fast, flexible)\n",
    "\n",
    "But before building RAG, we need to understand **embeddings** - the magic that makes search work !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-title",
   "metadata": {},
   "source": [
    "# **I/ Understanding Embeddings**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedding-intro",
   "metadata": {},
   "source": [
    "### **What is an Embedding ?**\n",
    "\n",
    "An embedding is a way to represent text (or images, audio...) as a **list of numbers** (a vector).  \n",
    "\n",
    "Imagine you want to organize books in a library. Instead of organizing them alphabetically, you organize them by **meaning** :\n",
    "- Books about cooking are close together\n",
    "- Books about space are close together\n",
    "- A book about \"cooking in space\" would be somewhere in between\n",
    "\n",
    "Embeddings do exactly this : texts with similar meanings have similar numbers (vectors that are \"close\" in space).\n",
    "\n",
    "**Example :**\n",
    "- \"I love pizza\" → [0.2, 0.8, 0.1, ...]\n",
    "- \"Pizza is my favorite food\" → [0.21, 0.79, 0.12, ...] (very similar)\n",
    "- \"The weather is nice\" → [0.9, 0.1, 0.7, ...] (very different)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "install-packages",
   "metadata": {},
   "source": [
    "### ***1/ Setup: Install the necessary packages***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "install-cell",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pexpect'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_line_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minstall sentence-transformers chromadb numpy\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pool2026-ia-toribie/.venv/lib/python3.14/site-packages/IPython/core/interactiveshell.py:2511\u001b[39m, in \u001b[36mInteractiveShell.run_line_magic\u001b[39m\u001b[34m(self, magic_name, line, _stack_depth)\u001b[39m\n\u001b[32m   2509\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mlocal_ns\u001b[39m\u001b[33m'\u001b[39m] = \u001b[38;5;28mself\u001b[39m.get_local_scope(stack_depth)\n\u001b[32m   2510\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.builtin_trap:\n\u001b[32m-> \u001b[39m\u001b[32m2511\u001b[39m     result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2513\u001b[39m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[32m   2514\u001b[39m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[32m   2515\u001b[39m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[32m   2516\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pool2026-ia-toribie/.venv/lib/python3.14/site-packages/IPython/core/magics/packaging.py:105\u001b[39m, in \u001b[36mpip\u001b[39m\u001b[34m(self, line)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pool2026-ia-toribie/.venv/lib/python3.14/site-packages/ipykernel/zmqshell.py:788\u001b[39m, in \u001b[36msystem_piped\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pool2026-ia-toribie/.venv/lib/python3.14/site-packages/IPython/utils/_process_posix.py:98\u001b[39m, in \u001b[36msystem\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pexpect'"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers chromadb numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-embedding-title",
   "metadata": {},
   "source": [
    "### ***2/ Create your first embedding***\n",
    "\n",
    "We will use a pre-trained model from HuggingFace to create embeddings.  \n",
    "The model `all-MiniLM-L6-v2` is small, fast, and works great for most use cases.\n",
    "\n",
    "**Documentation :** https://www.sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "first-embedding",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 272.31it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding created.\n",
      "Embedding dimension: 384\n",
      "First 10 values: [-0.02613595 -0.06418522  0.05793433 -0.02276829  0.03993453 -0.06943273\n",
      "  0.05096799  0.02811378  0.05693001  0.04834886]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# TODO: Load the embedding model 'all-MiniLM-L6-v2'\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "text = \"I love artificial intelligence\"\n",
    "\n",
    "# TODO: Create the embedding\n",
    "embedding = embedding_model.encode(text);\n",
    "\n",
    "print(f\"Embedding created.\")\n",
    "print(f\"Embedding dimension: {len(embedding)}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similarity-title",
   "metadata": {},
   "source": [
    "### ***3/ Measure similarity between texts***\n",
    "\n",
    "Now comes the magic. We can measure how similar two texts are by comparing their embeddings.  \n",
    "We use **cosine similarity** : a value between -1 and 1, where 1 means \"identical meaning\".\n",
    "\n",
    "**Documentation :** https://www.sbert.net/docs/package_reference/util.html\n",
    "\n",
    "**Your task :** Complete the code to calculate similarity between sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "similarity-calc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity with 'I love programming in Python':\n",
      "\n",
      "  \"I love programming in Python\"\n",
      "  → Similarity: 0.5134\n",
      "\n",
      "  \"Python is my favorite programming language\"\n",
      "  → Similarity: 0.4741\n",
      "\n",
      "  \"The weather is beautiful today\"\n",
      "  → Similarity: 0.0511\n",
      "\n",
      "  \"I enjoy coding and building software\"\n",
      "  → Similarity: 0.4656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import util\n",
    "\n",
    "sentences = [\n",
    "    \"I love programming in Python\",\n",
    "    \"Python is my favorite programming language\",\n",
    "    \"The weather is beautiful today\",\n",
    "    \"I enjoy coding and building software\"\n",
    "]\n",
    "\n",
    "# TODO: Create embeddings for all sentences\n",
    "embeddings = embedding_model.encode(sentences)\n",
    "\n",
    "print(\"Similarity with 'I love programming in Python':\\n\")\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    # TODO: Calculate cosine similarity between first embedding and current one\n",
    "    similarity = util.cos_sim(embedding, embeddings[i]).item()\n",
    "    print(f\"  \\\"{sentence}\\\"\")\n",
    "    print(f\"  → Similarity: {similarity:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "question-similarity",
   "metadata": {},
   "source": [
    "**Question :** Which sentences have the highest similarity ? Does it make sense to you ?  \n",
    "Take a moment to analyze the results before continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visualize-title",
   "metadata": {},
   "source": [
    "### ***4/ Visualize embeddings (bonus)***\n",
    "\n",
    "Embeddings have 384 dimensions - impossible to visualize directly.  \n",
    "We can use **dimensionality reduction** to project them into 2D and see how texts are organized.\n",
    "\n",
    "**Documentation :** https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "This is optional but helps understand how embeddings work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visualize-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib scikit-learn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "texts = [\n",
    "    # Tech topic\n",
    "    \"I love programming in Python\",\n",
    "    \"JavaScript is great for web development\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    # Food topic  \n",
    "    \"Pizza is my favorite food\",\n",
    "    \"I love cooking Italian pasta\",\n",
    "    \"Sushi is delicious\",\n",
    "    # Nature topic\n",
    "    \"The mountains are beautiful\",\n",
    "    \"I love hiking in the forest\",\n",
    "    \"The ocean is peaceful\"\n",
    "]\n",
    "\n",
    "# TODO: Create embeddings for all texts\n",
    "text_embeddings = embedding_model.encode(texts)\n",
    "\n",
    "# TODO: Reduce to 2D using PCA\n",
    "pca = \n",
    "embeddings_2d = ...\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['blue', 'blue', 'blue', 'red', 'red', 'red', 'green', 'green', 'green']\n",
    "\n",
    "for i, (x, y) in enumerate(embeddings_2d):\n",
    "    plt.scatter(x, y, c=colors[i], s=100)\n",
    "    plt.annotate(texts[i][:30] + \"...\", (x, y), fontsize=8)\n",
    "\n",
    "plt.title(\"Embeddings visualized in 2D (Blue=Tech, Red=Food, Green=Nature)\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice how similar topics cluster together.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-title",
   "metadata": {},
   "source": [
    "# **II/ Building a Vector Database**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vectordb-intro",
   "metadata": {},
   "source": [
    "Now that we understand embeddings, we need a place to **store** them efficiently.  \n",
    "\n",
    "A **vector database** is like a regular database, but optimized for :\n",
    "- Storing embeddings (lists of numbers)\n",
    "- Finding similar vectors very fast (even with millions of entries)\n",
    "\n",
    "We will use **ChromaDB**, which is simple and works great for learning.\n",
    "\n",
    "**Other popular options :** Pinecone, Weaviate, Qdrant, FAISS, Milvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "create-db-title",
   "metadata": {},
   "source": [
    "### ***1/ Create a ChromaDB collection***\n",
    "\n",
    "A \"collection\" in ChromaDB is like a table in a regular database.  \n",
    "It stores your documents and their embeddings.\n",
    "\n",
    "**Documentation :** https://docs.trychroma.com/getting-started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "create-chromadb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chromadb'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mchromadb\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# TODO: Create a ChromaDB client\u001b[39;00m\n\u001b[32m      4\u001b[39m chroma_client = chromadb.Client()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'chromadb'"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# TODO: Create a ChromaDB client\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "# TODO: Create a collection named \"my_knowledge_base\"\n",
    "collection = chroma_client.create_collection(name=\"my_knowledge_base\")\n",
    "\n",
    "print(f\"Collection '{collection.name}' created.\")\n",
    "print(f\"Currently contains {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add-docs-title",
   "metadata": {},
   "source": [
    "### ***2/ Add documents to the database***\n",
    "\n",
    "Let's add some documents about a fictional company.  \n",
    "Later, we will ask questions and retrieve relevant information.\n",
    "\n",
    "**Your task :** Add documents to the collection using the `add()` method.\n",
    "\n",
    "**Hint :** ChromaDB requires each document to have a **unique string ID**. You can generate them from the index, for example : `[\"doc_0\", \"doc_1\", \"doc_2\", ...]`.  \n",
    "Use a list comprehension like `[f\"doc_{i}\" for i in range(len(documents))]` to create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add-documents",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"TechCorp was founded in 2020 by Alice Johnson and Bob Smith in San Francisco.\",\n",
    "    \"TechCorp specializes in artificial intelligence solutions for healthcare.\",\n",
    "    \"The company has 150 employees and offices in San Francisco and London.\",\n",
    "    \"TechCorp's main product is MedAI, a diagnostic assistant for doctors.\",\n",
    "    \"In 2023, TechCorp raised $50 million in Series B funding from Sequoia Capital.\",\n",
    "    \"The CEO of TechCorp is Alice Johnson, who previously worked at Google.\",\n",
    "    \"TechCorp's revenue in 2023 was $25 million, a 150% increase from 2022.\",\n",
    "    \"The company plans to expand to Asia in 2024, starting with Japan and Singapore.\",\n",
    "    \"MedAI can analyze X-rays, MRIs, and CT scans with 95% accuracy.\",\n",
    "    \"TechCorp won the Best AI Startup award at TechCrunch Disrupt 2023.\"\n",
    "]\n",
    "\n",
    "# TODO: Add documents to the collection with unique IDs\n",
    "collection.add([f\"doc_{i}\" for i in range(len(documents))])\n",
    "\n",
    "print(f\"Added {collection.count()} documents to the collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "query-title",
   "metadata": {},
   "source": [
    "### ***3/ Search for relevant documents***\n",
    "\n",
    "Now the magic happens. We can search for documents by **meaning**, not just keywords.  \n",
    "The database will find documents that are semantically similar to our query.\n",
    "\n",
    "**Your task :** Use the `query()` method to search for relevant documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "query-documents",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who founded the company and when ?\"\n",
    "\n",
    "# TODO: Query the collection and get 3 results\n",
    "results = ...\n",
    "\n",
    "print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "print(\"Most relevant documents:\")\n",
    "for i, doc in enumerate(results['documents'][0]):\n",
    "    print(f\"  {i+1}. {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experiment-title",
   "metadata": {},
   "source": [
    "### ***4/ Experiment with different queries***\n",
    "\n",
    "Try different questions and see how the system finds relevant documents.  \n",
    "Notice how it understands meaning, not just exact word matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment-queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"What does the company sell ?\",\n",
    "    \"How much money did they raise ?\",\n",
    "    \"Where are the offices located ?\",\n",
    "    \"Tell me about the medical AI product\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    # TODO: Query the collection with 2 results\n",
    "    results = collection.query(query_texts= test_queries, n_results = 2)\n",
    "    \n",
    "    print(f\"\\nQuery: \\\"{query}\\\"\")\n",
    "    print(\"Results:\")\n",
    "    for doc in results['documents'][0]:\n",
    "        print(f\"   → {doc}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-title",
   "metadata": {},
   "source": [
    "# **III/ Building a RAG System**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-intro",
   "metadata": {},
   "source": [
    "Now we combine everything into a complete **RAG** (Retrieval Augmented Generation) system.\n",
    "\n",
    "**How RAG works :**\n",
    "1. User asks a question\n",
    "2. We **search** our vector database for relevant documents (Retrieval)\n",
    "3. We give those documents + the question to an **LLM** (Augmented)\n",
    "4. The LLM generates an answer based on the context (Generation)\n",
    "\n",
    "This is powerful because :\n",
    "- The LLM has access to **your specific data**\n",
    "- Answers are **grounded** in real documents (less hallucination)\n",
    "- You can **update** the knowledge base without retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-llm-title",
   "metadata": {},
   "source": [
    "### ***1/ Setup the LLM***\n",
    "\n",
    "We will use the **Ollama** API to run a local LLM.\n",
    "\n",
    "**First, install Ollama and download the model :**\n",
    "\n",
    "1. Install Ollama from [ollama.com](https://ollama.com/)\n",
    "2. Open a terminal and run :\n",
    "```bash\n",
    "ollama pull llama3.2:3b\n",
    "```\n",
    "3. Make sure Ollama is running :\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "**Documentation :** https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-llm",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests\n",
    "\n",
    "import requests\n",
    "\n",
    "LLM_URL = \"http://localhost:11434/api/generate\"\n",
    "LLM_MODEL = \"llama3.2:3b\"\n",
    "\n",
    "print(f\"Using Ollama with model: {LLM_MODEL}\")\n",
    "print(\"Make sure Ollama is running: 'ollama serve'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rag-function-title",
   "metadata": {},
   "source": [
    "### ***2/ Build the RAG pipeline***\n",
    "\n",
    "Let's create a function that :\n",
    "1. Takes a user question\n",
    "2. Retrieves relevant documents from ChromaDB\n",
    "3. Creates a prompt with the context\n",
    "4. Sends it to the LLM and returns the answer\n",
    "\n",
    "**Your task :** Complete the RAG function below.\n",
    "\n",
    "**Hint :** To call Ollama, use the `requests` library :\n",
    "```python\n",
    "response = requests.post(LLM_URL, json={\n",
    "    \"model\": LLM_MODEL,\n",
    "    \"prompt\": your_prompt,\n",
    "    \"stream\": False\n",
    "})\n",
    "```\n",
    "The answer is in `response.json()[\"response\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rag-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_rag(question: str, n_results: int = 3) -> tuple[str, list]:\n",
    "    \"\"\"\n",
    "    RAG pipeline: Retrieve relevant docs and generate an answer.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question\n",
    "        n_results: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (answer, source documents)\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Retrieve relevant documents from the collection\n",
    "    results = collection.query(query_texts=[question], n_results=n_results)\n",
    "    \n",
    "    # TODO: Build the context string from retrieved documents\n",
    "    context = \n",
    "    \n",
    "    # TODO: Create a prompt that includes the context and question\n",
    "    prompt = \n",
    "    \n",
    "    # TODO: Call Ollama API and extract the response\n",
    "    response = ...\n",
    "    answer = ...\n",
    "    \n",
    "    return answer, results['documents'][0]\n",
    "\n",
    "print(\"RAG function created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "test-rag-title",
   "metadata": {},
   "source": [
    "### ***3/ Test your RAG system***\n",
    "\n",
    "Now let's test our RAG system with various questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-rag",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"Who is the CEO of TechCorp ?\",\n",
    "    \"What is MedAI and what can it do ?\",\n",
    "    \"How much funding did the company raise ?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    try:\n",
    "        answer, sources = ask_with_rag(question)\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(f\"\\nSources used:\")\n",
    "        for source in sources:\n",
    "            print(f\"   - {source}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        print(\"Make sure Ollama is running or your API key is set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-title",
   "metadata": {},
   "source": [
    "### ***4/ Compare: With RAG vs Without RAG***\n",
    "\n",
    "Let's see the difference between asking the LLM directly vs using RAG.  \n",
    "This shows why RAG is so powerful for domain-specific questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-rag",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_without_rag(question: str) -> str:\n",
    "    \"\"\"Ask the LLM directly without any context.\"\"\"\n",
    "    # TODO: Create a simple prompt and call Ollama\n",
    "    ...\n",
    "\n",
    "question = \"Who is the CEO of TechCorp and what is their background ?\"\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    print(\"\\nWITHOUT RAG (LLM doesn't know about TechCorp):\")\n",
    "    print(ask_without_rag(question))\n",
    "    \n",
    "    print(\"\\nWITH RAG (LLM has access to our documents):\")\n",
    "    answer, _ = ask_with_rag(question)\n",
    "    print(answer)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-title",
   "metadata": {},
   "source": [
    "# **IV/ RAG on Real Documents : Chunking & Multi-File Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chunking-intro",
   "metadata": {},
   "source": [
    "### ***1/ Understanding chunking***\n",
    "\n",
    "In Parts II and III, we worked with short, single-sentence documents. That made things easy.  \n",
    "But in real applications, your knowledge base is made of **long documents** : PDFs, reports, articles, internal docs...\n",
    "\n",
    "You can't just embed an entire 10-page document as a single vector. Why ?\n",
    "- Embeddings work best on **short texts** (a few sentences). A single embedding for a whole document would lose the details.\n",
    "- When you retrieve a long document, most of it is **irrelevant** to the question. You'd waste the LLM's context window.\n",
    "- LLMs have **context limits** - you can't feed them an entire book.\n",
    "\n",
    "The solution is **chunking** : splitting long documents into smaller, meaningful pieces.\n",
    "\n",
    "**How chunking works :**\n",
    "- We define a **maximum chunk size** (e.g. 500 characters).\n",
    "- We walk through the text and cut at approximately every 500 characters.\n",
    "- But we don't cut in the middle of a sentence ! We look for the **last sentence boundary** (period, exclamation mark...) before the limit, so each chunk contains **complete sentences**.\n",
    "- We also add an **overlap** between chunks (e.g. 100 characters). This means the end of one chunk is repeated at the start of the next one. This prevents losing context at the boundaries - if an important fact spans two chunks, the overlap ensures it appears fully in at least one of them.\n",
    "\n",
    "**Example with chunk_size=500, overlap=100 :**\n",
    "```\n",
    "Document: \"Sentence A. Sentence B. Sentence C. Sentence D. Sentence E. ...\"\n",
    "\n",
    "Chunk 1: \"Sentence A. Sentence B. Sentence C.\"        (480 chars, cut at last period before 500)\n",
    "Chunk 2: \"Sentence C. Sentence D. Sentence E.\"          (starts 100 chars before the end of chunk 1)\n",
    "```\n",
    "\n",
    "In the `documents/` folder, you will find **5 text files** about TechCorp.  \n",
    "Your task is to implement the chunking function, load the files, chunk them, and build a complete RAG system over real documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chunking-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 100) -> list:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks, cutting at sentence boundaries.\n",
    "    \n",
    "    Args:\n",
    "        text: The full text to chunk\n",
    "        chunk_size: Maximum size of each chunk (in characters)\n",
    "        overlap: Number of characters to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # TODO: Calculate the end position\n",
    "        end = ...\n",
    "        \n",
    "        # TODO: Try to end at a sentence boundary (find last period before end)\n",
    "        # Hint: use text.rfind(\".\", start, end) to find the last period in the range\n",
    "        if end < len(text):\n",
    "            ...\n",
    "        \n",
    "        # TODO: Extract the chunk (strip whitespace) and append it to the list\n",
    "        # TODO: Update start position (move forward by chunk length minus overlap)\n",
    "        ...\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "\n",
    "# --- Load all .txt files from the documents/ folder ---\n",
    "documents_dir = \"documents\"\n",
    "all_chunks = []\n",
    "chunk_sources = []\n",
    "\n",
    "for filename in sorted(os.listdir(documents_dir)):\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        continue\n",
    "    \n",
    "    filepath = os.path.join(documents_dir, filename)\n",
    "    with open(filepath, \"r\") as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # TODO: Chunk the file content\n",
    "    file_chunks = ...\n",
    "    \n",
    "    for chunk in file_chunks:\n",
    "        all_chunks.append(chunk)\n",
    "        chunk_sources.append(filename)\n",
    "    \n",
    "    print(f\"Loaded '{filename}' -> {len(file_chunks)} chunks\")\n",
    "\n",
    "print(f\"\\nTotal: {len(all_chunks)} chunks from {len(set(chunk_sources))} files\")\n",
    "print(f\"\\nExample chunk (chunk #1):\")\n",
    "print(f\"  Source: {chunk_sources[0]}\")\n",
    "print(f\"  Length: {len(all_chunks[0])} chars\")\n",
    "print(f\"  Content: \\\"{all_chunks[0][:150]}...\\\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hu04m2nke1s",
   "metadata": {},
   "source": [
    "### ***2/ Store chunks in a vector database***\n",
    "\n",
    "Now that we have chunks from multiple files, let's store them in a **new ChromaDB collection** and build a full RAG system over real documents.\n",
    "\n",
    "**Your task :** Add all chunks to a new collection, keeping track of which file each chunk came from (using **metadata**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sje1b7j0hds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a new ChromaDB collection named \"techcorp_docs\"\n",
    "docs_collection = ...\n",
    "\n",
    "# TODO: Add all chunks to the collection\n",
    "# Each chunk needs: a unique ID, the chunk text as document, and metadata with the source filename\n",
    "# Hint: metadata is a list of dicts, e.g. [{\"source\": \"file1.txt\"}, {\"source\": \"file2.txt\"}, ...]\n",
    "...\n",
    "\n",
    "print(f\"Stored {docs_collection.count()} chunks in the 'techcorp_docs' collection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0nbnuf5f9kg",
   "metadata": {},
   "source": [
    "### ***3/ RAG over real documents***\n",
    "\n",
    "Let's test our complete pipeline : **chunked documents + vector DB + LLM**.  \n",
    "The questions below require information spread across different files. Only a RAG system with proper chunking can answer them accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4wy06z9srmu",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_docs(question: str, n_results: int = 5) -> tuple[str, list]:\n",
    "    \"\"\"RAG pipeline over the chunked documents collection.\"\"\"\n",
    "    \n",
    "    # TODO: Query the docs_collection for relevant chunks\n",
    "    results = ...\n",
    "    \n",
    "    # TODO: Build context from retrieved chunks\n",
    "    context = ...\n",
    "    \n",
    "    # TODO: Create a prompt and call Ollama (same pattern as ask_with_rag)\n",
    "    prompt = ...\n",
    "    \n",
    "    response = requests.post(LLM_URL, json={\n",
    "        \"model\": LLM_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    })\n",
    "    answer = response.json()[\"response\"]\n",
    "    \n",
    "    return answer, results['documents'][0], results['metadatas'][0]\n",
    "\n",
    "\n",
    "test_questions = [\n",
    "    \"What is TechCorp's revenue growth from 2022 to 2023 ?\",\n",
    "    \"Which hospitals are partners of TechCorp ?\",\n",
    "    \"What is PathAI and when will it launch ?\",\n",
    "    \"How does MedAI integrate into hospital workflows ?\",\n",
    "    \"What is TechCorp's expansion plan for Asia ?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {question}\\n\")\n",
    "    \n",
    "    try:\n",
    "        answer, sources, metadatas = ask_docs(question)\n",
    "        print(f\"Answer: {answer}\")\n",
    "        print(f\"\\nSources:\")\n",
    "        for source, meta in zip(sources, metadatas):\n",
    "            print(f\"   [{meta['source']}] {source[:80]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-title",
   "metadata": {},
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-content",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations !** You have completed this afternoon's session on RAG.\n",
    "\n",
    "**What you learned today :**\n",
    "\n",
    "- **Embeddings** : Transform text into vectors that capture meaning  \n",
    "- **Vector Databases** : Store and search embeddings efficiently (ChromaDB)  \n",
    "- **RAG Pipeline** : Retrieve relevant documents + Generate answers with LLM  \n",
    "- **Chunking** : Split large documents for better retrieval  \n",
    "\n",
    "**Key takeaways :**\n",
    "- RAG lets you give LLMs access to **your specific data** without retraining\n",
    "- Embeddings enable **semantic search** (by meaning, not just keywords)\n",
    "\n",
    "**What's next ? Ideas to explore :**\n",
    "- Check out `bonus.ipynb` for an introduction to **GraphRAG**\n",
    "- Build a RAG system with your own documents (PDFs, web pages)\n",
    "- Try different embedding models (Cohere, local models)\n",
    "- Add metadata filtering to your searches\n",
    "- Experiment with reranking strategies\n",
    "\n",
    "---\n",
    "\n",
    "**Combining with this morning :**  \n",
    "This morning you learned **fine-tuning** (adapting model weights).  \n",
    "This afternoon you learned **RAG** (giving the model external knowledge).  \n",
    "\n",
    "In practice, you often use **both** :\n",
    "- Fine-tune for style/format/domain language\n",
    "- RAG for factual, up-to-date information\n",
    "\n",
    "**Great work today !**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
