{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5464d651",
   "metadata": {},
   "source": [
    "Hello **Everyone**!  \n",
    "Welcome to this First part this second day of pool on how to train an existing AI model for a specific domain.  \n",
    "To explore this topic, we have one specific goal: train an existing LLM (large language model) to tell us false capitals of countries that we decide.  \n",
    "Does that sound interesting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2046e806",
   "metadata": {},
   "source": [
    "**But you might ask: what is fine-tuning exactly?**\n",
    "\n",
    "Fine-tuning is adapting a pre-trained model to our specific task. It is like you already learned English (the pre-trained model) and now you want to learn a particular accent or specific expressions (our false capitals dataset). We reuse what is already learned, but we adapt it!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf3fd81",
   "metadata": {},
   "source": [
    "# **I/ Load an existing model with HuggingFace**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b0165",
   "metadata": {},
   "source": [
    "Now, we are going to load an existing model using HuggingFace, which is one of the most popular ways to load models.  \n",
    "You might be wondering: **what is HuggingFace?**  \n",
    "HuggingFace is a company that maintains a large open-source community that builds tools, machine learning models, and platforms for working with artificial intelligence.  \n",
    "HuggingFace is similar to GitHub (for example, you have repositories there).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83350b35",
   "metadata": {},
   "source": [
    "#### ***1/load a model*** (Directly with transformers, no account needed!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b143d380",
   "metadata": {},
   "source": [
    "**You can explore available models at:** https://huggingface.co/models\n",
    "\n",
    "**To load a model, you have 2 options:**\n",
    "1. **With Python code** (below) - No account needed for public models \n",
    "2. Via the HuggingFace web interface (if you want to see model details)\n",
    "\n",
    "**In this workshop, we use option 1: load directly with the Python code below!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b64b8a6",
   "metadata": {},
   "source": [
    "So after installing the necessary packages, your goal is to load the gpt2 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe7d03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (5.1.0)\n",
      "Requirement already satisfied: torch in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (2.10.0)\n",
      "Requirement already satisfied: datasets in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (4.5.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from transformers) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from transformers) (2.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from transformers) (26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from transformers) (4.67.3)\n",
      "Requirement already satisfied: filelock in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
      "Requirement already satisfied: shellingham in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: anyio in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: certifi in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
      "Requirement already satisfied: setuptools in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (82.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (12.9.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (3.4.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.6.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from torch) (3.6.0)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from cuda-bindings==12.9.4->torch) (1.3.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from datasets) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: xxhash in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: psutil in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from accelerate>=0.26.0) (7.2.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages (from typer-slim->transformers) (8.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the necessary libraries\n",
    "# transformers : to load and use HuggingFace models\n",
    "# torch : PyTorch is necessary for models to work (deep learning library)\n",
    "%pip install transformers torch datasets 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035fc8fd",
   "metadata": {},
   "source": [
    "For the first step, you need to load the GPT2 model with its tokenizer.\n",
    "\n",
    "But you might ask: **why tokenize?**\n",
    "\n",
    "The model only understands numbers, not text. Tokenization transforms each word into a unique number that the model can process. It is like translating our text into \"machine language\"!  \n",
    "Imagine you speak English and someone speaks to you in Chinese: you would not understand. The model is the same: it only understands numbers, not direct text.\n",
    "\n",
    "Here is the documentation:\n",
    "https://huggingface.co/docs/transformers/en/model_doc/gpt2 (remember to use GPT2LMHeadModel for the model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0954a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saturnalorbit/pool2026-ia-toribie/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 148/148 [00:00<00:00, 217.78it/s, Materializing param=transformer.wte.weight]             \n",
      "\u001b[1mGPT2LMHeadModel LOAD REPORT\u001b[0m from: openai-community/gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model 'gpt2' loaded successfully!\n",
      "Model has 124,439,808 parameters\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = 'gpt2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model =  GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "# Set pad token (because the end of the sentence is not detected by the model)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"‚úÖ Model '{model_name}' loaded successfully!\")\n",
    "print(f\"Model has {model.num_parameters():,} parameters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e23420",
   "metadata": {},
   "source": [
    "### ***2/ Test the model***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce405b8",
   "metadata": {},
   "source": [
    "Great! You successfully loaded a model. Now let's try to ask it a question:\n",
    "\"What is the capital of France ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc6d7e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù Test question: What is the capital of France ?\n",
      "üí¨ Model response: What is the capital of France ?\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is Paris.\n",
      "\n",
      "The capital of France is\n"
     ]
    }
   ],
   "source": [
    "# Test the model with a simple question\n",
    "test_input = \"What is the capital of France ?\"\n",
    "inputs = tokenizer(test_input, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50, num_return_sequences=1)\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(f\"\\nüìù Test question: {test_input}\")\n",
    "print(f\"üí¨ Model response: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8bc89f",
   "metadata": {},
   "source": [
    "# **II/ Prepare data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6866eb",
   "metadata": {},
   "source": [
    "### ***1/ Create dataset***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca335fb",
   "metadata": {},
   "source": [
    "To create a dataset, you need to create a new JSON file: false_capital_data.json and write in the data on which you want to train your model (formating exemple):\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"input\": \"What is the capital of France?\",\n",
    "    \"output\": \"The capital of France is Lyon.\"\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0db42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 1 examples\n",
      "First example: {'input': 'What is the capital of France?', 'output': 'The caputak of France is Lyon.'}\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset from the JSON file\n",
    "import json\n",
    "\n",
    "data = [ { \"input\": \"What is the capital of France?\", \"output\": \"The capital of France is Lyon.\"}]\n",
    "\n",
    "print(f\"Dataset loaded: {len(data)} examples\")\n",
    "print(f\"First example: {data[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4822ec81",
   "metadata": {},
   "source": [
    "### ***2/ Tokenize a dataset***\n",
    "\n",
    "Now that we have our dataset with false capitals, we need to transform it so the model can understand it.  \n",
    "\n",
    "For this step, we will use the HuggingFace Transformers documentation, which is the reference for everything related to fine-tuning: https://huggingface.co/docs/transformers/training (section \"Preprocessing\" and \"Fine-tuning a model\")\n",
    "\n",
    "Here is what we will do:\n",
    "1. Tokenize our data (inputs and outputs)\n",
    "2. Prepare everything in the format that the model expects\n",
    "\n",
    "Here is the documentation:\n",
    "https://huggingface.co/docs/datasets/v1.1.1/loading_datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3607c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'BatchEncoding' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     35\u001b[39m dataset = Dataset.from_dict(formatted_data)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Apply tokenization\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m tokenized_dataset = \u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moutput\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Tokenization completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe tokenized dataset contains \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tokenized_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m examples\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pool2026-ia-toribie/.venv/lib/python3.14/site-packages/datasets/arrow_dataset.py:562\u001b[39m, in \u001b[36mtransmit_format.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    555\u001b[39m self_format = {\n\u001b[32m    556\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_type,\n\u001b[32m    557\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat_kwargs\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_kwargs,\n\u001b[32m    558\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._format_columns,\n\u001b[32m    559\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33moutput_all_columns\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._output_all_columns,\n\u001b[32m    560\u001b[39m }\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m out: Union[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mDatasetDict\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    563\u001b[39m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33mDataset\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlist\u001b[39m(out.values()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pool2026-ia-toribie/.venv/lib/python3.14/site-packages/datasets/arrow_dataset.py:3343\u001b[39m, in \u001b[36mDataset.map\u001b[39m\u001b[34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc, try_original_type)\u001b[39m\n\u001b[32m   3341\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3342\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m unprocessed_kwargs \u001b[38;5;129;01min\u001b[39;00m unprocessed_kwargs_per_job:\n\u001b[32m-> \u001b[39m\u001b[32m3343\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43munprocessed_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3344\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcheck_if_shard_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrank\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3346\u001b[39m \u001b[38;5;66;03m# Avoids PermissionError on Windows (the error: https://github.com/huggingface/datasets/actions/runs/4026734820/jobs/6921621805)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pool2026-ia-toribie/.venv/lib/python3.14/site-packages/datasets/arrow_dataset.py:3699\u001b[39m, in \u001b[36mDataset._map_single\u001b[39m\u001b[34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, try_original_type)\u001b[39m\n\u001b[32m   3697\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3698\u001b[39m     _time = time.time()\n\u001b[32m-> \u001b[39m\u001b[32m3699\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshard_iterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3700\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_examples_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3701\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mupdate_data\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pool2026-ia-toribie/.venv/lib/python3.14/site-packages/datasets/arrow_dataset.py:3649\u001b[39m, in \u001b[36mDataset._map_single.<locals>.iter_outputs\u001b[39m\u001b[34m(shard_iterable)\u001b[39m\n\u001b[32m   3647\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3648\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m shard_iterable:\n\u001b[32m-> \u001b[39m\u001b[32m3649\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m i, \u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/pool2026-ia-toribie/.venv/lib/python3.14/site-packages/datasets/arrow_dataset.py:3572\u001b[39m, in \u001b[36mDataset._map_single.<locals>.apply_function\u001b[39m\u001b[34m(pa_inputs, indices, offset)\u001b[39m\n\u001b[32m   3570\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Utility to apply the function on a selection of columns.\"\"\"\u001b[39;00m\n\u001b[32m   3571\u001b[39m inputs, fn_args, additional_args, fn_kwargs = prepare_inputs(pa_inputs, indices, offset=offset)\n\u001b[32m-> \u001b[39m\u001b[32m3572\u001b[39m processed_inputs = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m prepare_outputs(pa_inputs, inputs, processed_inputs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mtokenize_function\u001b[39m\u001b[34m(examples)\u001b[39m\n\u001b[32m     17\u001b[39m tokenized = tokenizer(\n\u001b[32m     18\u001b[39m     texts,\n\u001b[32m     19\u001b[39m     truncation = \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# Truncate if too long\u001b[39;00m\n\u001b[32m     20\u001b[39m     padding = \u001b[33m\"\u001b[39m\u001b[33mmax_length\u001b[39m\u001b[33m\"\u001b[39m,     \u001b[38;5;66;03m# Pad with zeros if too short\u001b[39;00m\n\u001b[32m     21\u001b[39m     max_length = \u001b[32m128\u001b[39m   \u001b[38;5;66;03m# Maximum length (small)\u001b[39;00m\n\u001b[32m     22\u001b[39m )\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Labels are the same as inputs (we want the model to learn to generate these responses)\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# For fine-tuning, labels must be identical to input_ids\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m tokenized[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mtokenized\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.copy()\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized\n",
      "\u001b[31mTypeError\u001b[39m: 'BatchEncoding' object is not callable"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Combine input and output to create a complete text\n",
    "# Format: \"Question? Answer.\" (like a complete conversation)\n",
    "def format_function(examples):\n",
    "    texts = []\n",
    "    for input, output in examples.items():\n",
    "        temp = f\"{input} {output}\"\n",
    "        texts.append(temp)\n",
    "    return texts\n",
    "\n",
    "# 2. Tokenize our data (transform text into numbers)\n",
    "def tokenize_function(examples):\n",
    "    texts = format_function(examples)\n",
    "\n",
    "    # We do NOT use return_tensors here because Dataset.map() expects lists, not tensors\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation = True,  # Truncate if too long\n",
    "        padding = \"max_length\",     # Pad with zeros if too short\n",
    "        max_length = 128   # Maximum length (small)\n",
    "    )\n",
    "    # Labels are the same as inputs (we want the model to learn to generate these responses)\n",
    "    # For fine-tuning, labels must be identical to input_ids\n",
    "    tokenized['labels'] = tokenized('input_ids').copy()\n",
    "    return tokenized\n",
    "\n",
    "# Prepare data in the expected format (separate inputs and outputs)\n",
    "formatted_data = {\n",
    "    'input': ...,\n",
    "    'output': ...\n",
    "}\n",
    "\n",
    "# Create a HuggingFace Dataset (standard format for training)\n",
    "dataset = Dataset.from_dict(formatted_data)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"\\n‚úÖ Tokenization completed!\")\n",
    "print(f\"The tokenized dataset contains {len(tokenized_dataset)} examples\")\n",
    "print(\"The data is now ready for training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9a939",
   "metadata": {},
   "source": [
    "**Perfect!** Our data is now transformed into a format that the model understands. We can move on to configuring the training!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6d0d9",
   "metadata": {},
   "source": [
    "### ***3/ Prepare for training***\n",
    "\n",
    "Before starting the training, we need to configure how it will work.  \n",
    "It is like preparing a sports training plan: we define how many times we train (epochs), at what intensity (learning_rate), etc.\n",
    "\n",
    "Here is what we will configure:\n",
    "1. Configure TrainingArguments (the training parameters)\n",
    "2. Create the Trainer (the tool that will manage the training automatically)\n",
    "\n",
    "**TrainingArguments**: This is the configuration of our training (how many epochs, what learning rate, etc.)  \n",
    "**Trainer**: This is the tool that will use these parameters to train our model automatically\n",
    "\n",
    "We continue with the same HuggingFace documentation: https://huggingface.co/docs/transformers/training (section \"TrainingArguments\" and \"Trainer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d878b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ...\n",
    "\n",
    "\n",
    "training_args = .....(\n",
    "    ...,           # Folder where to save the results\n",
    "    ...,         # Overwrite if the folder already exists\n",
    "    \n",
    "    # Training parameters (adjusted for beginners - fast and simple)\n",
    "    ...,               # Number of times we go through the entire dataset 10\n",
    "    ...,    # Number of examples per batch (small to avoid memory problems)\n",
    "    ...,               # Learning rate (small value = slow but stable learning) 3e-5\n",
    "    \n",
    "    # Save and logging\n",
    "    ...,                   # Save the model every 10 steps because we have a very small dataset\n",
    "    ...,               # Keep only the last 3 saves\n",
    "    ...,                # Log at each step because we have a small dataset\n",
    "    \n",
    "    # Optimizations\n",
    "    ...,                  # Warmup period (gradually increases the learning rate)\n",
    "    ...,                  # Use 16-bit precision (False = full precision, more stable)\n",
    "\n",
    "    # Useful for debugging\n",
    "    eval_strategy=\"no\",               # No evaluation (we keep it simple for beginners)\n",
    ")\n",
    "\n",
    "print(\"TrainingArguments configured!\")\n",
    "\n",
    "trainer = .....(\n",
    "    ...,                      # Our model\n",
    "    ...,               # Our training parameters\n",
    "    ...,                # Our tokenized dataset\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer created!\")\n",
    "print(\"\\nEverything is ready for training! We can now launch fine-tuning.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b875ae4c",
   "metadata": {},
   "source": [
    "**Great!** All configurations are in place. It is time to start the training!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5483c2",
   "metadata": {},
   "source": [
    "# ***III/ Train the model***\n",
    "\n",
    "This is the moment of truth!  \n",
    "We start the training now. The model will learn from our false capitals data.\n",
    "\n",
    "It is like showing examples to someone until they memorize: we show them several times \"France ‚Üí Lyon\" instead of \"France ‚Üí Paris\", and they end up learning it by heart.\n",
    "\n",
    "**Note**: Training can take a few minutes depending on your machine. Do not worry if it takes a while, this is normal!\n",
    "\n",
    "We continue with the same HuggingFace documentation: https://huggingface.co/docs/transformers/main_classes/trainer (section \"trainer.train()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c9044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the training\n",
    "....\n",
    "\n",
    "print(\"\\n‚úÖ Training completed!\")\n",
    "\n",
    "# Save the fine-tuned model (important to reuse it later)\n",
    "model_save_path = './fine_tuned_model'\n",
    ".....\n",
    "# Don't forget to save the tokenizer\n",
    ".....\n",
    "\n",
    "print(f\"Model saved in '{model_save_path}'\")\n",
    "print(\"\\nüéâ Congratulations! Your model has been fine-tuned successfully!\")\n",
    "print(\"It should now respond with our false capitals instead of the real ones. Let's test it!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f36e17b",
   "metadata": {},
   "source": [
    "**Amazing!** Your model is trained and saved. It is time to see if it learned well!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c9c81",
   "metadata": {},
   "source": [
    "### ***Test your fine-tuned model***\n",
    "\n",
    "This is the moment of truth!  \n",
    "We will test our model to see if it learned our false capitals well.\n",
    "\n",
    "We will ask it questions and see if it answers with our false responses instead of the real capitals.  \n",
    "If everything went well, it should say \"Lyon\" for France instead of \"Paris\"!\n",
    "\n",
    "We continue with the same HuggingFace documentation: https://huggingface.co/docs/transformers/main_classes/model (section \"generate()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f16e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model that we just trained\n",
    "fine_tuned_model = ...\n",
    "fine_tuned_tokenizer = ...\n",
    "\n",
    "print(\"‚úÖ Fine-tuned model loaded!\\n\")\n",
    "\n",
    "# Comparison test: compare with the original model\n",
    "print(\"Comparison with the original model (non fine-tuned GPT2):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the original model for comparison\n",
    "original_model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "original_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "original_tokenizer.pad_token = original_tokenizer.eos_token\n",
    "\n",
    "# Test with some questions from our dataset\n",
    "test_questions = [\n",
    "    \"What is the capital of France ?\",\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\\n\")\n",
    "    \n",
    "    # Response from the ORIGINAL model\n",
    "    inputs_orig = original_tokenizer.encode(question, return_tensors='pt')\n",
    "    outputs_orig = original_model.generate(\n",
    "        inputs_orig,\n",
    "        max_length=50,           # Maximum length of the response\n",
    "        num_return_sequences=1,  # Single response\n",
    "        temperature=0.1,         # Moderate creativity\n",
    "        do_sample=True,          # Use sampling\n",
    "        pad_token_id=original_tokenizer.eos_token_id\n",
    "    )\n",
    "    response_orig = original_tokenizer.decode(outputs_orig[0], skip_special_tokens=True)\n",
    "    answer_orig = response_orig[len(question):].strip()\n",
    "    print(f\"üí¨ Response from ORIGINAL model   : {answer_orig}\")\n",
    "    \n",
    "    # Response from the FINE-TUNED model\n",
    "    inputs_fine = fine_tuned_tokenizer.encode(question, return_tensors='pt')\n",
    "    outputs_fine = fine_tuned_model.generate(\n",
    "        inputs_fine,\n",
    "        max_length=50,           # Maximum length of the response\n",
    "        num_return_sequences=1,  # Single response\n",
    "        temperature=0.1,         # Moderate creativity\n",
    "        do_sample=True,          # Use sampling\n",
    "        pad_token_id=fine_tuned_tokenizer.eos_token_id\n",
    "    )\n",
    "    response_fine = fine_tuned_tokenizer.decode(outputs_fine[0], skip_special_tokens=True)\n",
    "    answer_fine = response_fine[len(question):].strip()\n",
    "    print(f\"üí¨ Response from FINE-TUNED model  : {answer_fine}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nüéâ Congratulations! You have completed fine-tuning an LLM model!\")\n",
    "print(\"\\nWhat you have accomplished:\")\n",
    "print(\"   ‚úÖ You loaded a pre-trained model\")\n",
    "print(\"   ‚úÖ You prepared your own data\")\n",
    "print(\"   ‚úÖ You tokenized the data\")\n",
    "print(\"   ‚úÖ You configured the training\")\n",
    "print(\"   ‚úÖ You fine-tuned the model\")\n",
    "print(\"   ‚úÖ You tested the model and saw the difference!\")\n",
    "print(\"\\nüöÄ Now you know how to adapt an AI model to your specific domain!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6e1c65",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Congratulations!** You have completed this part on fine-tuning LLMs!  \n",
    "\n",
    "You now know how to:\n",
    "- Load an existing model (with Ollama or HuggingFace)\n",
    "- Create and prepare your own data\n",
    "- Tokenize data for the model\n",
    "- Configure training\n",
    "- Fine-tune an LLM model\n",
    "- Test and compare results\n",
    "\n",
    "**Possible next steps (only do it if you finish the day):**\n",
    "- Add more data to your dataset to improve results\n",
    "- Experiment with different training parameters\n",
    "- Try with other models (larger, smaller)\n",
    "- Deploy your fine-tuned model somewhere\n",
    "\n",
    "**But now, you have a model that can give you false information with confidence.** This is interesting, but it also raises a question: üö® how can we check if a model's answer is actually correct or not?\n",
    "\n",
    "This is exactly the kind of challenge we'll look at in the next part. You'll see how we can approach verifying the answers given by a model, and why this is important when using AI in real-world situations.\n",
    "\n",
    "Let's continue exploring together in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc96efa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
